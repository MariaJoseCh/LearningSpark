{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea0cf8b6",
   "metadata": {},
   "source": [
    "## Schema and blogs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c6bc31e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "import org.apache.spark.sql.types._\r\n",
       "import org.apache.spark.sql.functions._\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99839271",
   "metadata": {},
   "source": [
    "Spark allows you to define a schema in two ways.\n",
    "One is to define it programmatically.\n",
    "The other is to employ a Data Definition Language (DDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb9f4b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(author,StringType,false),StructField(title,StringType,false),StructField(pages,IntegerType,false))\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//A) Define a schema programmatically for a DataFrame with three named columns: authot, title and pages. We can use the Spark DataFrame API.\n",
    "val schema = StructType(Array(StructField(\"author\", StringType, false), StructField(\"title\", StringType, false), StructField(\"pages\", IntegerType, false)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deb46e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema: String = author STRING, title STRING, pages INT\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// B) Defining the same schema using DDL is much simpler:\n",
    "val schema = \"author STRING, title STRING, pages INT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20f6419d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@26098db4\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// You can choose whichever way you like to define a schema. For many examples, we will use both.\n",
    "//Same code than Example 3.2. (Python) but now in Scala and this time reading from a JSON file:\n",
    "val spark = SparkSession\n",
    " .builder\n",
    " .appName(\"Example_3.2.\")\n",
    " .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a85900d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jsonFile: String = C:/Users/mariajose.chinchilla/OneDrive - Bosonit/Escritorio/Bosonit/Spark/datarepositorio/chapter3/data/blogs.json\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Get the path to the JSON file\n",
    " val jsonFile = \"C:/Users/mariajose.chinchilla/OneDrive - Bosonit/Escritorio/Bosonit/Spark/datarepositorio/chapter3/data/blogs.json\"\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0d31a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(Id,IntegerType,false),StructField(First,StringType,false),StructField(Last,StringType,false),StructField(Url,StringType,false),StructField(Published,StringType,false),StructField(Hits,IntegerType,false),StructField(Campaigns,ArrayType(StringType,true),false))\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Define our schema programmatically\n",
    " val schema = StructType(Array(StructField(\"Id\", IntegerType, false),\n",
    " StructField(\"First\", StringType, false),\n",
    " StructField(\"Last\", StringType, false),\n",
    " StructField(\"Url\", StringType, false),\n",
    " StructField(\"Published\", StringType, false),\n",
    " StructField(\"Hits\", IntegerType, false),\n",
    " StructField(\"Campaigns\", ArrayType(StringType), false)))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37a01713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "blogsDF: org.apache.spark.sql.DataFrame = [Id: int, First: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a DataFrame by reading from the JSON file \n",
    " // with a predefined schema\n",
    " val blogsDF = spark.read.schema(schema).json(jsonFile)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8701829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+----------------------------+\n",
      "|Id |First    |Last   |Url              |Published|Hits |Campaigns                   |\n",
      "+---+---------+-------+-----------------+---------+-----+----------------------------+\n",
      "|1  |Jules    |Damji  |https://tinyurl.1|1/4/2016 |4535 |[twitter, LinkedIn]         |\n",
      "|2  |Brooke   |Wenig  |https://tinyurl.2|5/5/2018 |8908 |[twitter, LinkedIn]         |\n",
      "|3  |Denny    |Lee    |https://tinyurl.3|6/7/2019 |7659 |[web, twitter, FB, LinkedIn]|\n",
      "|4  |Tathagata|Das    |https://tinyurl.4|5/12/2018|10568|[twitter, FB]               |\n",
      "|5  |Matei    |Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB, LinkedIn]|\n",
      "|6  |Reynold  |Xin    |https://tinyurl.6|3/2/2015 |25568|[twitter, LinkedIn]         |\n",
      "+---+---------+-------+-----------------+---------+-----+----------------------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "// Show the DataFrame schema as output\n",
    " blogsDF.show(false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e9ef364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- First: string (nullable = true)\n",
      " |-- Last: string (nullable = true)\n",
      " |-- Url: string (nullable = true)\n",
      " |-- Published: string (nullable = true)\n",
      " |-- Hits: integer (nullable = true)\n",
      " |-- Campaigns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "()\n",
      "StructType(StructField(Id,IntegerType,true),StructField(First,StringType,true),StructField(Last,StringType,true),StructField(Url,StringType,true),StructField(Published,StringType,true),StructField(Hits,IntegerType,true),StructField(Campaigns,ArrayType(StringType,true),true))\n"
     ]
    }
   ],
   "source": [
    "// Print the schema\n",
    " println(blogsDF.printSchema)\n",
    " println(blogsDF.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cccd6ab",
   "metadata": {},
   "source": [
    "Let's take a look at some examples of what we can do with columns in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ea278db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Array[String] = Array(Id, First, Last, Url, Published, Hits, Campaigns)\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogsDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc8e661f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: org.apache.spark.sql.Column = Id\r\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Access a particular column with col and it returns a Column type\n",
    "blogsDF.col(\"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2503a842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "// Use an expression to compute a value\n",
    "blogsDF.select(expr(\"Hits * 2\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93702a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "// or use col to compute value\n",
    "blogsDF.select(col(\"Hits\") * 2).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac700682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|Big Hitters|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|      false|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|      false|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|      false|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|       true|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|       true|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|       true|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "// Use an expression to compute big hitters for blogs\n",
    "// This adds a new column, Big Hitters, based on the conditional expression\n",
    "blogsDF.withColumn(\"Big Hitters\", (expr(\"Hits > 10000\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5e8ce7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|    AuthorsId|\n",
      "+-------------+\n",
      "|  JulesDamji1|\n",
      "| BrookeWenig2|\n",
      "|    DennyLee3|\n",
      "|TathagataDas4|\n",
      "+-------------+\n",
      "only showing top 4 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "// Concatenate three columns, create a new column, and show the\n",
    "// newly created concatenated column\n",
    "blogsDF\n",
    ".withColumn(\"AuthorsId\", (concat(expr(\"First\"), expr(\"Last\"), expr(\"Id\"))))\n",
    " .select(col(\"AuthorsId\"))\n",
    " .show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c2ee160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// These statements return the same value, showing that expr is the same as a col method call\n",
    "blogsDF.select(expr(\"Hits\")).show(2)\n",
    "blogsDF.select(col(\"Hits\")).show(2)\n",
    "blogsDF.select(\"Hits\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58f96238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Sort by column \"Id\" in descending order\n",
    "blogsDF.sort(col(\"Id\").desc).show()\n",
    "blogsDF.sort($\"Id\".desc).show()\n",
    "//In this last example, the expressions blogs_df.sort(col(\"Id\").desc) and blogsDF.sort($\"Id\".desc) are identical. They both sort the DataFrame column named Id in descending order: one uses an explicit function, col(\"Id\"), to return a Column object, while the other uses $ before the name of the column, which is a function in Spark that converts column named Id to a Column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59b758f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\r\n",
       "blogRow: org.apache.spark.sql.Row = [6,Reynold,Xin,https://tinyurl.6,255568,3/2/2015,[Ljava.lang.String;@7a4ca4f9]\r\n",
       "res13: Any = Reynold\r\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// ROWS: A row in Spark is a generic Row object, containing one or more columns. \n",
    "import org.apache.spark.sql.Row\n",
    "// Create a Row\n",
    "val blogRow = Row(6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", 255568, \"3/2/2015\", Array(\"twitter\", \"LinkedIn\"))\n",
    "// Access using index for individual items\n",
    "blogRow(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8983de85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|       Author|State|\n",
      "+-------------+-----+\n",
      "|Matei Zaharia|   CA|\n",
      "|  Reynold Xin|   CA|\n",
      "+-------------+-----+\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rows: Seq[(String, String)] = List((Matei Zaharia,CA), (Reynold Xin,CA))\r\n",
       "authorsDF: org.apache.spark.sql.DataFrame = [Author: string, State: string]\r\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Row objects can be used to create DataFrames.\n",
    "val rows = Seq((\"Matei Zaharia\", \"CA\"), (\"Reynold Xin\", \"CA\"))\n",
    "val authorsDF = rows.toDF(\"Author\", \"State\")\n",
    "authorsDF.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
