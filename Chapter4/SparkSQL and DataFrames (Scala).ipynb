{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a33577a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "//DATA: departuredelays.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8178e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@47d9638\r\n",
       "csvFile: String = C:/Users/mariajose.chinchilla/OneDrive - Bosonit/Escritorio/Bosonit/Spark/datarepositorio/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\r\n",
       "df: org.apache.spark.sql.DataFrame = [date: int, delay: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Let's get started by reading the data set into a temporary view.\n",
    "import org.apache.spark.sql.SparkSession \n",
    "val spark = SparkSession\n",
    " .builder\n",
    " .appName(\"SparkSQLExampleApp\")\n",
    " .getOrCreate()\n",
    "\n",
    "// Path to data set \n",
    "val csvFile=\"C:/Users/mariajose.chinchilla/OneDrive - Bosonit/Escritorio/Bosonit/Spark/datarepositorio/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    "\n",
    "// Read and create a temporary view\n",
    "// Infer schema (note that for larger files you may want to specify the schema)\n",
    "val df = spark.read.format(\"csv\")\n",
    " .option(\"inferSchema\", \"true\")\n",
    " .option(\"header\", \"true\")\n",
    " .load(csvFile)\n",
    "\n",
    "//Create a temporary view\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d35557e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema: String = date STRING, delay INT, distance INT, origin STRING, destination STRING\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// If you want to specify a schema, you can use a DDL-formatted string\n",
    "val schema = \"date STRING, delay INT, distance INT, origin STRING, destination STRING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d816cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Now that we have a temporary view, we can issue SQL queries using Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c27a482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "//We will find all flights whose distance is greater than 1000 miles.\n",
    "spark.sql(\"\"\"SELECT distance, origin, destination \n",
    "FROM us_delay_flights_tbl WHERE distance > 1000 \n",
    "ORDER BY distance DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5889df05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|2190925| 1638|   SFO|        ORD|\n",
      "|1031755|  396|   SFO|        ORD|\n",
      "|1022330|  326|   SFO|        ORD|\n",
      "|1051205|  320|   SFO|        ORD|\n",
      "|1190925|  297|   SFO|        ORD|\n",
      "|2171115|  296|   SFO|        ORD|\n",
      "|1071040|  279|   SFO|        ORD|\n",
      "|1051550|  274|   SFO|        ORD|\n",
      "|3120730|  266|   SFO|        ORD|\n",
      "|1261104|  258|   SFO|        ORD|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "//Find all flights between San Francisco (SFO) and Chicago (ORD) with at least a twho-hour delay\n",
    "spark.sql(\"\"\"SELECT date, delay, origin, destination \n",
    "FROM us_delay_flights_tbl \n",
    "WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD' \n",
    "ORDER by delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd8b5ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "//Label all US flights, regardless of origin and destination, with an indication of the delays they experienced: Very Long Delays (>6 hours), Long Delays (2-6 hours), etc. We will add these human-readable labels in a new column called Flight_Delays:\n",
    "spark.sql(\"\"\"SELECT delay, origin, destination,\n",
    " CASE\n",
    " WHEN delay > 360 THEN 'Very Long Delays'\n",
    " WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n",
    " WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n",
    " WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'\n",
    " WHEN delay = 0 THEN 'No Delays'\n",
    " ELSE 'Early'\n",
    " END AS Flight_Delays\n",
    " FROM us_delay_flights_tbl\n",
    " ORDER BY origin, delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b55a6041",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " Hive support is required to CREATE Hive TABLE (AS SELECT).;\r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: Hive support is required to CREATE Hive TABLE (AS SELECT).;\r",
      "'CreateTable `spark_catalog`.`learn_spark_db`.`managed_us_delay_flights_tbl`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists\r",
      "\r",
      "  at org.apache.spark.sql.errors.QueryCompilationErrors$.ddlWithoutHiveSupportEnabledError(QueryCompilationErrors.scala:1743)\r",
      "  at org.apache.spark.sql.execution.datasources.HiveOnlyCheck$.$anonfun$apply$4(rules.scala:438)\r",
      "  at org.apache.spark.sql.execution.datasources.HiveOnlyCheck$.$anonfun$apply$4$adapted(rules.scala:435)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreach(TreeNode.scala:285)\r",
      "  at org.apache.spark.sql.execution.datasources.HiveOnlyCheck$.apply(rules.scala:435)\r",
      "  at org.apache.spark.sql.execution.datasources.HiveOnlyCheck$.apply(rules.scala:433)\r",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$53(CheckAnalysis.scala:747)\r",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$53$adapted(CheckAnalysis.scala:747)\r",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:747)\r",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:160)\r",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)\r",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:156)\r",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:146)\r",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)\r",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)\r",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\r",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)\r",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)\r",
      "  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\r",
      "  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r",
      "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\r",
      "  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:640)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r",
      "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:630)\r",
      "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:671)\r",
      "  ... 38 elided\r",
      ""
     ]
    }
   ],
   "source": [
    "//Create both a managed and a managed and an unmanaged table.\n",
    "\n",
    "//To begin, we'll create a database called learn_spark_db and tell Spark we want to use that database\n",
    "spark.sql(\"CREATE DATABASE learn_spark_db\")\n",
    "spark.sql(\"USE learn_spark_db\")\n",
    "\n",
    "//Create a managed table within the database learn_spark_db\n",
    "spark.sql(\"CREATE TABLE managed_us_delay_flights_tbl (date STRING, delay INT, distance INT, origin STRING, destination STRING)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
