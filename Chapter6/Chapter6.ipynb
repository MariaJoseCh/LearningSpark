{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f56b75a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.avg\r\n",
       "import org.apache.spark.sql.SparkSession\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.avg\n",
    "import org.apache.spark.sql.SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f419207b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@492a6e56\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a DataFrame using SparkSession\n",
    "val spark = SparkSession\n",
    " .builder\n",
    " .appName(\"Chapter6\")\n",
    " .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02a6149d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Bloggers\r\n",
       "bloggers: String = C:/Users/mariajose.chinchilla/OneDrive - Bosonit/Escritorio/Bosonit/Spark/datarepositorio/databricks-datasets/learning-spark-v2/blogs.json\r\n",
       "bloggersDS: org.apache.spark.sql.Dataset[Bloggers] = [Campaigns: array<string>, First: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//To create a distributed Dataset[Bloggers], we define a Scala case class that defines each individual field that comprises a Scala object\n",
    "case class Bloggers(id:BigInt, first:String, last:String, url:String, published:String, hits:BigInt, campaigns:Array[String])\n",
    "\n",
    "//We read the file from the data source\n",
    "val bloggers = \"C:/Users/mariajose.chinchilla/OneDrive - Bosonit/Escritorio/Bosonit/Spark/datarepositorio/databricks-datasets/learning-spark-v2/blogs.json\"\n",
    "val bloggersDS = spark\n",
    "  .read\n",
    "  .format(\"json\")\n",
    "  .option(\"path\", bloggers)\n",
    "  .load()\n",
    "  .as[Bloggers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f312baaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+\n",
      "|uid|     uname|usage|\n",
      "+---+----------+-----+\n",
      "|  0|user-Gpi2C|  525|\n",
      "|  1|user-DgXDi|  502|\n",
      "|  2|user-M66yO|  170|\n",
      "|  3|user-xTOn6|  913|\n",
      "|  4|user-3xGSz|  246|\n",
      "|  5|user-2aWRN|  727|\n",
      "|  6|user-EzZY1|   65|\n",
      "|  7|user-ZlZMZ|  935|\n",
      "|  8|user-VjxeG|  756|\n",
      "|  9|user-iqf1P|    3|\n",
      "+---+----------+-----+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import scala.util.Random._\r\n",
       "defined class Usage\r\n",
       "r: scala.util.Random = scala.util.Random@1a3c2840\r\n",
       "data: scala.collection.immutable.IndexedSeq[Usage] = Vector(Usage(0,user-Gpi2C,525), Usage(1,user-DgXDi,502), Usage(2,user-M66yO,170), Usage(3,user-xTOn6,913), Usage(4,user-3xGSz,246), Usage(5,user-2aWRN,727), Usage(6,user-EzZY1,65), Usage(7,user-ZlZMZ,935), Usage(8,user-VjxeG,756), Usage(9,user-iqf1P,3), Usage(10,user-91S1q,794), Usage(11,user-qHNj0,501), Usage(12,user-7hb94,460), Usage(13,user-bz0WF,142), Usage(14,user-71nwy,479), Usage(15,user-7GZz1,823), Usage(16,user-1CSk6,140), Usage(17,user-WPzlL,246), Usage(18,user-VaEit,451), Usage(19,user-PSaRq,679), Usage(20,user-0Kkzu,332), Usage(21,user-UN3MG,172), Usage(22,user-KwwER,442), Usage(23,user-ZnltJ,923), Usage(24,user-IRA17,741), ...\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Create a Scala object with three fields:uid(unique ID for a user), uname(randomly generated username string) and usage(minutes of server or service usage)\n",
    "\n",
    "import scala.util.Random._\n",
    "\n",
    "// Our case class for the Dataset\n",
    "case class Usage(uid:Int, uname:String, usage: Int)\n",
    "\n",
    "val r = new scala.util.Random(42)\n",
    "\n",
    "// Create 1000 instances of scala Usage class \n",
    "// This generates data on the fly\n",
    "val data = for (i <- 0 to 1000)\n",
    " yield (Usage(i, \"user-\" + r.alphanumeric.take(5).mkString(\"\"),\n",
    " r.nextInt(1000)))\n",
    "\n",
    "// Create a Dataset of Usage typed data\n",
    "val dsUsage = spark.createDataset(data)\n",
    "dsUsage.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9575ec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "//Filter to return all the users in our dsUsage dataset whose usage exceeds 900 minutes. \n",
    "\n",
    "//One way to do this is to use a functional expressions as an argument to the filter method\n",
    "import org.apache.spark.sql.functions._\n",
    "dsUsage\n",
    "  .filter(d => d.usage > 900)\n",
    "  .orderBy(desc(\"usage\"))\n",
    "  .show(5, false)\n",
    "\n",
    "//We use a lambda expression {d.usage > 900} as an argument to the filter() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e300508",
   "metadata": {},
   "outputs": [],
   "source": [
    "//Another way is to define a function and supply that function as an argument to filter()\n",
    "def filterWithUsage(u: Usage) = u.usage > 900\n",
    "dsUsage.filter(filterWithUsage(_)).orderBy(desc(\"usage\")).show(5)\n",
    "\n",
    "//We define a Scala function def filterWithUsage(u:Usage)= u.usage > 900\n",
    "\n",
    "//In both cases, the filter() method iterates over each row of the Usage object in the distributed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e7e4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "//We want to find out the usage cost for each user whose usage value is over a certain threshold so we can offer those users a special price per minute\n",
    "\n",
    "// Use an if-then-else lambda expression and compute a value\n",
    "dsUsage.map(u => {if (u.usage > 750) u.usage * .15 else u.usage * .50 })\n",
    " .show(5, false)\n",
    "// Define a function to compute the usage\n",
    "def computeCostUsage(usage: Int): Double = {\n",
    " if (usage > 750) usage * 0.15 else usage * 0.50\n",
    "}\n",
    "// Use the function as an argument to map()\n",
    "dsUsage.map(u => {computeCostUsage(u.usage)}).show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d02a760",
   "metadata": {},
   "outputs": [],
   "source": [
    "//We have computed values for the cost of usage, we donâ€™t know which users the computed values are associated with. How do we get this information?\n",
    "\n",
    "//1. Create a Scala case class or JavaBean class, UsageCost, with an additional field or column named cost.\n",
    "//2. Define a function to compute the cost and use it in the map() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2272e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create a new case class with an additional field, cost\n",
    "case class UsageCost(uid: Int, uname:String, usage: Int, cost: Double)\n",
    "// Compute the usage cost with Usage as a parameter\n",
    "// Return a new object, UsageCost\n",
    "def computeUserCostUsage(u: Usage): UsageCost = {\n",
    " val v = if (u.usage > 750) u.usage * 0.15 else u.usage * 0.50\n",
    " UsageCost(u.uid, u.uname, u.usage, v)\n",
    "}\n",
    "// Use map() on our original Dataset\n",
    "dsUsage.map(u => {computeUserCostUsage(u)}).show(5)\n",
    "\n",
    "//Now we have a transformed Dataset with a new column, cost, computed by the function in our map() transformation, along with all the other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9921ef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "//To convert an existing DataFrame df to a Dataset of type SomeCaseClass\n",
    "val bloggersDS = spark\n",
    " .read\n",
    " .format(\"json\")\n",
    " .option(\"path\", \"/FileStore/tables/blogs-1.json\")\n",
    " .load()\n",
    " .as[Bloggers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcebadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "//Suppose we have a Dataset of type Person, where Person is defined as a Scala case class:\n",
    "case class Person(id: Integer, firstName: String, middleName:String, lastName: String, gender: String, birthDate: String, ssn: String, salary: String)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c63c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "/Examine a case where we compose a query inefficiently:\n",
    "\n",
    "/*\n",
    "\n",
    "import java.util.Calendar\n",
    "val earliestYear = Calendar.getInstance.get(Calendar.YEAR) - 40\n",
    "Person\n",
    " // Everyone above 40: lambda-1\n",
    " .filter(x => x.birthDate.split(\"-\")(0).toInt > earliestYear)\n",
    " \n",
    " // Everyone earning more than 80K\n",
    " .filter($\"salary\" > 80000)\n",
    "\n",
    "// Last name starts with J: lambda-2\n",
    " .filter(x => x.lastName.startsWith(\"J\"))\n",
    " \n",
    " // First name starts with D\n",
    " .filter($\"firstName\".startsWith(\"D\"))\n",
    " .count()\n",
    "\n",
    "/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dad8947",
   "metadata": {},
   "outputs": [],
   "source": [
    "//The following query uses only DSL and no lambdas\n",
    "/*\n",
    "personDS\n",
    " .filter(year($\"birthDate\") > earliestYear) // Everyone above 40\n",
    " .filter($\"salary\" > 80000) // Everyone earning more than 80K\n",
    " .filter($\"lastName\".startsWith(\"J\")) // Last name starts with J\n",
    " .filter($\"firstName\".startsWith(\"D\")) // First name starts with D\n",
    " .count()\n",
    "\n",
    "/*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
